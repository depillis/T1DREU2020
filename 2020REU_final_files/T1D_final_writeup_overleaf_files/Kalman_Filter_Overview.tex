\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1.0in]{geometry}
\usepackage[bottom]{footmisc}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{tcolorbox}
\usepackage{subcaption}
\usepackage{float}
\usepackage{multirow}
\usepackage[toc,page]{appendix}
\usepackage{adjustbox}
\usepackage{setspace}



\title{Kalman Filter Overview}
\author{subteam 2}
\date{June 2020}

\begin{document}

\maketitle

\section{A General Overview of Kalman Filtering}
Kalman Filters (KF) fall under the umbrella of Machine Learning models, however are additionally an example of a mechanistic model, meaning that it allows us to gain an understanding of the individual, inner workings of complex systems. Moreover, unlike MCMC's, which are used on already fully obtained data sets, Kalman Filters can be adjusted as new data is brought in. Rather than making predictions using an entire dataset at once, KFs improve and evolve as more datapoints are assessed. To use a KF, the system must be in a discretized state space form \cite{SimonHaykinText}. To do this, one must identify two sets of variables: latent states and observables.\\
\\
Latent states are the set of variables that one is interested in estimating, which can include values such as a population count or model parameters. On the other hand, observables are a set of data which has been measured from the system and is the only piece of information known about the system at a particular time point. \\
\\
Additionally, Kalman Filters are helpful due to their ability to quantify the noise associated with a system. This is achieved through both process and measurement noise. Process noise describes outside forces and situations which, as a result of their actions, affect states in the model. Measurement noise, on the other hand, accounts for the fact that tools to gather observable data may be inherently flawed, and thus some level of noise is associated with the values they produce. In order to gather a holistic, high level understanding of Kalman Filters, consider the example of a car driving down a street.\\
\\
Assume that one observes a car at time $t$ traveling at a velocity of 5 m/s at a position of 2 m from an observed starting point. In order to estimate what the car's velocity will be at time $t+1$, one may choose to utilize a Kalman Filter. Assuming that the position at time $t+1$ can be measured to be 5 m with a device, this position thus is the observable, while the velocity is a latent state. However, since the tool is flawed, measurement noise is added in as well. If we assume that while this is occurring, a 25 mph wind blows from behind the car, this would now become a source of process noise. Having all of this information, one is now capable of applying a KF to estimate the velocity of the car at time $t+1$.\\
\\
As stated earlier, Kalman Filters operate in discrete time, state space models, meaning that input and output variables are being related to one another through a system of ODEs at known time points. Thus, we must have knowledge about the system to the extent that we know the structure of the ODEs, although as we'll see, there are formats of the Kalman Filter in which it is not necessary to know the parameter values. Having both an understanding of the ODEs and the observable data, we can now introduce the the projection and update steps, the two of which drive the KF algorithm. 
%{The simplest use of Kalman Filters is for state estimation, which we will describe in the following sections. Although our eventual goal is to implement parameter estimation, since parameter estimation is based on the foundation of state estimation, it is easier to understand if you first have a general understanding of state estimation. As a result of this, we will introduce  parameter estimation later on.%} 
\subsection{Projection Step}
The first step of the KF relies only on knowledge of the system. Having an estimate of latent states at time $t$, we use our knowledge of the system to predict the states at time $t + 1$, known as the prior estimate.
\subsection{Update Step}
Having a prior estimate, we can now bring in the observable data. By seeing how the prediction we made in the Projection Step compares to the observable data, we now update our predictions and states. This is now our posterior estimate, and becomes the set of states that are projected forward to produce the prior estimate for time $t+2$. We continue this loop as long as new data is coming in, continously improving results as more data arrives. Figure X shows this two step process visually. \\
\\
\begin{figure}[H]
    \centering
    \includegraphics[width=15cm]{Kalman Filter Images/Kalman_Filter_Simple_Diagram.png}
    \caption{The two steps of the Kalman Filter algorithm. First, we use knowledge of the system to create an initial prediction, following which observable data is produced to produce final estimates.}

\end{figure}

\section{Introduction to Kalman Filters}
    Please note that all notation and equations in this section are adapted from \cite{VanMereChapter} and \cite{SimonHaykinText}, two sources we have found very helpful in guiding our understanding of Kalman Filters.
    \subsection{Different Types of Kalman Filters}
    Multiple formulations of Kalman Filters exist for different types of systems. When a system is linear, one may use the standard Linear Kalman Filter. However, once the system becomes nonlinear this is no longer feasible. If the nonlinear system may be accurately linearized with first order Taylor approximations, the Extended Kalman Filter (EKF) may be used, the theory for which is based around the first order Taylor approximation. However, if the nonlinear system  is not easily linearized, it is recommended to use the Unscented Kalman Filter (UKF) \cite{VanMereChapter}. Due to the inability of complex ODE systems to be accurately approximated with first order Taylor approximations, we will focus on the Unscented Kalman Filter here.
    
    \subsection{Process and Measurement Equations}
    The Unscented Kalman Filter is used to estimate the states of a nonlinear system. The system, set up as a state space model, is described by the process and measurement equations \cite{SimonHaykinText}. The process equation, describing the transition of the latent states from time $k$ to time $k+1$ is formulated as
    \begin{equation}
    x_{k+1} = F_{k+1,k}(x_k) + q_k, 
    \end{equation}
    where $x_{k}$ is the vector of states at time $k$, $x_{k+1}$ is the vector of states at time $k+1$, $F$ is the non-linear transition function that takes the state from time $k$ to time $k+1$, and $q_k$ is the process noise \cite{SimonHaykinText}. $F$ and $q_k$ are assumed to be known quantities. The Measurement equation, describing the relationship between observed data points and the unknown states, is 
    \begin{equation}
    y_k = H_k(x_k) + r_k,  
    \end{equation}
    where $y_k$ is the vector of the observables, $H$ is a (possibly) nonlinear function that relates the states to the observables, and $r_k$ represents the measurement noise \cite{SimonHaykinText}. The role of $H$ is a nuance one that is not always fully explained in literature. Often times, only some sort of algebraic combination of states can be observed. For example, if the latent state is simply $x$, we may only be able to observe $3x^2$. Thus, the purpose of $H$ is to relate the actual observed value, $3x^2$ to the latent state $x$.\\
    \\
    $H$ and $r_k$ are assumed to be known in the Measurement equation. 
    \subsection{Set up of the Noise}
    The above system of equations relies on both process and measurement noise. The vector $q_k$, which follows the same dimensions as the state vector in order to apply one noise value to each state, describes the process noise and is multiviate Gaussian with mean 0 and covariance described by \cite{SimonHaykinText}
    \begin{equation}
    E[q_{n}q_{k}^T] = \Bigg\{\begin{tabular}{l}
    $Q_k$\ for\ n = k  \\
    0   for \ $n \neq k$
    \end{tabular}.
    \end{equation}
    
    Similarly, $r_k$ describes the measurement noise and is Gaussian with mean 0 and covariance described by \cite{SimonHaykinText}
    \begin{equation}
    E[r_{n}r_{k}^T] = \Bigg\{\begin{tabular}{l}
    $R_k$\ for\ n = k  \\
    0   for \ $n \neq k$
    \end{tabular}. 
    \end{equation}
    
    
    Both of these covariance matrices will play a role in the UKF calculations. Furthermore, it is important to note that our system assumes white, additive noise. If we have a system where the noise is not additive, the Kalman Filter equations must be altered, which can be seen in \cite{VanMereChapter}. 
    
    \subsection{Unscented Transformation}
    The Unscented Kalman Filter is formulated around the Unscented Transformtation (UT). The UT is used to understand the distribution of a random variable after a nonlinear transformation has been applied to it. To do this, the UT creates a set of \textbf{sigma vectors} meant to capture points in the distribution in an ideal, deterministic fashion. \cite{VanMereChapter} Consider random variable $v$, i.e.
    
    \begin{equation}
    v = \begin{bmatrix} v_1\\ v_2\\v_3\\.\\.\\.\\v_L\end{bmatrix},
    \end{equation}
    
    with dimensions $L$ x $1$. Additionally, consider a different random variable $z$ that is some function of $v$ such that $z = f(v)$. Also assume that the distribution of $v$ has mean $\bar{v}$ and covariance $P_v$. To understand the distribution of the nonlinear transformation y, we create a $L$ x $2L+1$ matrix $\chi$ of 2$L$ + 1 sigma vectors as follows:
    \begin{equation} \label{eq:UT_1}
    \chi_0 = \bar{v}
    \end{equation}
    \begin{equation} \label{eq:UT_2}
    \chi_i = \bar{v} + (\sqrt{(L + \lambda)P_v})_i \ i = 1 ... L
    \end{equation}
    \begin{equation} \label{eq:UT_3}
    \chi_i = \bar{v} - (\sqrt{(L + \lambda)P_v})_{i - L} \ i = L + 1 \  ...  \ 2L + 1.
    \end{equation}
    The number 2$L$+1 gives us one vector at the mean, and $L$ vectors above and below the mean. As stated above, $L$ is the dimension of $v$ which means it is number of states in our system. The subscript $i$ refers to the $i$th columns in the matrix square root of the term $(\sqrt{(L + \lambda)P_v})$.\\
    \\
    Here, $\lambda$ is a final scaling parameter defined as $\alpha^2(L + k) - L$, where $\alpha$ is a scaling parameter that represents the spread of vectors around $\bar{v}$. $k$ is a secondary scaling parameter, set to either 0 or $3-L$, whereas $\alpha$ takes on a value between 10e-4 and 1 \cite{VanMereChapter}. It is crucial to note that this is a form of \textbf{deterministic} scaling. In many sampling schemes, sampling is done \textbf{randomly} from a distribution. However, here our goal is to choose points in equal intervals and in equal quantities of either side of the mean, $\bar{v}$. The purpose of this is so that we may be certain that sigma points accurately capture the spread of a distribution. With random sampling, if only choosing a small amount of points, we can make no guarantees about the distribution of these points. Thus, exact equations are given for how the sigma vectors should be chosen. \\
    \\
    Once $\chi$ has been calculated, the sigma vectors are then sent through the nonlinear function $f$ to create vectors $Z_i$. This is done as follows:
    \begin{equation}
    Z_i = f(\chi_i) \ \ i = 0,...,2L.
    \end{equation}
    
    Now, by weighting these output vectors $Z_i$ in a deterministic fashion, we can calculate a mean and covariance in order to understand our new distribution. \cite{VanMereChapter} In order to do this, we create the following weights, where $W^{(m)}$ are weights corresponding to calculating the mean and $W^{(c)}$ is used to calculate our covariance:
    \begin{equation} \label{eq:Weights_1}
    W_0^{(m)} = \lambda/(L + \lambda),
    \end{equation}
    \begin{equation} \label{eq:Weights_2}
    W_0^{(c)} = \lambda/(L + \lambda) + (1 - \alpha^2 + \beta),
    \end{equation}
    \begin{equation} \label{eq:Weights_3}
    W_i^{(m)} = W_i^{(c)} = 1/(2(L + \lambda)) \ i = 1 ... 2L.
    \end{equation}
    Here, $\beta$ holds prior knowledge about the distribution (for Gaussian $\beta = 2$ is used) \cite{VanMereChapter}. We can see that the sigma point that represents the mean is given more weight than the others. Now the mean and covariance can be calculated:
    \begin{equation}
     \bar{z} \approx \sum_{i = 0}^{2L} W_i^{(m)} Z_i,
    \end{equation}
    \begin{equation}
    P_z \approx \sum_{i = 0}^{2L} W_i^{(c)}(Z_i - \bar{z})(Z_i - \bar{z})^T.    
    \end{equation}
    Understanding this mean and covariance will be useful in creating our prior estimates in the UKF method.
    \\
    \\
    For a visual representation, see Figure \ref{fig:KF_Overview_WanMereImage}, adapted from \cite{VanMereChapter}:\\
    \begin{figure} [H]
    \centering
    \includegraphics[scale = .5]{Kalman Filter Images/Chapter7ImageUpdated.jpg}
    \caption{A figure comparing the Unscented Transformation (right) with a standard sampling procedure (left). The standard sampling procedure samples the distribution of $x$ extensively, and then transforms the sampled points  using the transformation function $f$ to get the mean and covariance of the transformed distribution, $y$. The UT uses sigma points to capture the distribution of $x$, and then passes them through the transformation function $f$ to get the approximation of the distribution of $y$. Although the UT used substantially less points than the standard sampling method, it still manages to get a close approximation of the mean and covariance of the system.}
    \label{fig:KF_Overview_WanMereImage}
    \end{figure}
    Here, on the left hand side we see a standard sampling procedure where the distribution of $x$ is sampled extensively and the sampled points are then passed through the transformation function $f$ to get the distribution of $y$. On the right, we see the same sampling procedure using the Unscented Transformation (UT), where we instead create a deterministic sample of sigma points from the distribution of $x$ and apply the transformation function $f$ to them. The sigma points that are chosen are indicated by the red circles. Then, using a weighting schema, we are able to use the transformed sigma points to find a sample mean and covariance for the distribution of $y$. We can see that the Unscented Transformation process, even though it only uses a select number of points, is able to closely the true mean and covariance as presented on the left. 
    
    \section{Kalman Filter Description}
    
    \subsection{Notation}
    Before going deeper into the process of using a Kalman Filter, it is important to establish the notation that will be used throughout. This notation is adapted from the Simon-Haykin textbook, Kalman Filtering and Neural Networks. \cite{SimonHaykinText}
    \subsubsection{States Notation}
    For our states, we first have $\hat{x}_0$ for our initial posterior estimate. Next, we notate our prior estimate as $\hat{x}_k^-$ and our posterior estimate, following the Update Step, as $\hat{x}_k$. Then, we have analogous notation for the states covariance. Namely, we start with $P_0$, have a prior estimate of $P_k^-$, and then a posterior covariance of $P_k$.
    \subsubsection{Observables Notation}
    We must define similar notation for our observables. Here, we have a prior estimate $\hat{y}_k^-$, but instead of a posterior estimates, we have the \emph{actual} observable value $y_k$. Using these two values, we also define the error, or \emph{innovation}, as:
    \begin{equation}
    \tilde{y}_k = y_k - \hat{y}_k^-.    
    \end{equation}
    Then, we once again have a covariance matrix, but this time for the innovation, $P_{\tilde{y}_k}$. This is the basis of the notation, however more will be introduced as necessary in the following sections, and a full guide can be found in Table \ref{table:KF_Notation}.
    
    \subsection{Initialization}
    The initial states vector $x_0$ and the initial covariance matrix between the states, $P_0$, are needed to initialize the UKF. To create these initial guesses some prior knowledge of the states and their plausible ranges is necessary. Assuming this information is available, one may initialize with:
    \begin{equation}
    \hat{x}_0 = E[x_0],
    \end{equation}
    and initial covariance matrix:
    \begin{equation}
    P_0 = E[(x_0 - \hat{x}_0)(x_0 - \hat{x}_0)^T].
    \end{equation}
    \cite{VanMereChapter}
    When working with real data, one is not likely to have a full understanding of the states' distribution. Thus, a first guess will suffice and may be tuned throughout working with the UKF, as will be discussed in the \textbf{Kalman Filter Implementation} section.  
    \subsection{Projection Step}
    Now, we proceed to do the projection portion of the algorithm. \cite{VanMereChapter} Having chosen a set of sigma points via equations \ref{eq:Weights_1}, \ref{eq:Weights_2}, and \ref{eq:Weights_3} , we now project them in time by applying the transition matrix to each point in the vector $\chi_{k-1}$:
    \begin{equation}
    \chi_{k|k-1} = F(\chi_{k-1}).
    \end{equation}
    It is important to note that here $\hat{x}_{k-1}$ plays the role of $\bar{v}$ in equations \ref{eq:UT_1}, \ref{eq:UT_2}, and \ref{eq:UT_3}.
    Next we calculate a prior prediction of the state $\hat{x}_k^-$ with:
    \begin{equation}
    \hat{x}_k^- = \sum_{i = 0}^{2L} W_i^{(m)} \chi_{i, k|k-1}.
    \end{equation}
    Similarly we can calculate a prior covariance $P_k^-$ by using our weights as well as the process noise:
    \begin{equation}
    P_k^- = \sum_{i = 0}^{2L} W_i^{(c)} [\chi_{i, k|k-1} - \hat{x}_k^-][\chi_{i, k|k-1} - \hat{x}_k^-]^T + Q.
    \end{equation}
    We can see that the calculation of $\hat{x}_k^-$ and $P_k^-$ are both applications of the Unscented Transformation. Next, we apply the UT transformation to understand our observables with:
    \begin{equation}
    Y_{k|k-1} = H[\chi_{k|k-1}].
    \end{equation}
    This allows us to know calculate a prior estimate of the observables $y_k^-$ by utilizing:
    \begin{equation}
    \hat{y}_k^- = \sum_{i=0}^{2L} W_i^{(m)} Y_{i,k|k-1}.
    \end{equation}
     Once again, we have made use of the UT transformation here.
    \subsection{Update Step}
    Having created prior estimates for both states and observables, we now proceed to the update step where the observed data is brought in. \cite{VanMereChapter} \\
    \\
    We begin by calculating the covariance matrix $P_{\tilde{y}_k, \tilde{y}_k}$, which is the covariance matrix for the error between the projected and actual values of the observables. \\
    \\
    Moreover, this calculation takes into account the measurement noise through $R$:
    \begin{equation}
    P_{\tilde{y}_k, \tilde{y}_k} = \sum_{i=0}^{2L} W_i^{(c)} [Y_{i,k|k-1} - \hat{y}_k^-][Y_{i,k|k-1} - \hat{y}_k^-]^T + R. 
    \end{equation}
    Next we need a covariance between our states and observables, which is calculated by:
    \begin{equation}
    P_{{x}_k,{y}_k} = \sum_{i=0}^{2L} W_i^{(c)} [\chi_{i,k|k-1} - \hat{x}_k^-][Y_{i,k|k-1} - \hat{y}_k^-]^T.
    \end{equation}
    At this point, we have sufficient information to calculate the \textbf{Kalman Gain}, a crucial term in the UKF algorithm. The Kalman Gain is used to weight the error between the predicted value and the observed value of $x_k$ in order to update the filter's prior predictions. A larger Kalman Gain will give more weight to the error, resulting in a larger correction and a smaller Kalman Gain will give less weight to the error, resulting in a smaller correction. The Kalman Gain is also used to adjust the covariance matrix $P_k$ and is calculated through \cite{VanMereChapter}:
    \begin{equation}
    K_k = P_{x_k, y_k} P_{\tilde{y}_k, \tilde{y}_k}^{-1}. 
    \end{equation}
    Calculation of the Kalman Gain allows us to calculate our posterior estimates of both the states, $\hat{x}_k$, and covariance, $P_k$.\cite{VanMereChapter} For the states the following equation is used
    \begin{equation}
    \hat{x}_k = \hat{x}_k^- + K_k(y_k - \hat{y}_k^-). 
    \end{equation}
    And for the covariance we use:
    \begin{equation} \label{eq:27ukf}
    P_k = P_k^-  - K_k P_{\tilde{y}_k, \tilde{y}_k} K_k^T.
    \end{equation}
    We can see that both equations function by adjusting our prior prediction through use of the Kalman Gain.\\
    \\
    After having gone through the update step, the next data point would be brought in and we would return to the Projection Step once more.
    \subsection{Summary}
    A flow chart depicting one iteration of the entire UKF process is found in figure \ref{fig:UKF_Theory_FlowChart}. To summarize, our goal is to begin with a set of sigma points chosen in a deterministic fashion and arrive at the posterior estimates for the states and covariance. This is done in two main sections, the Projection Step, whose quantities are shaded in red, and the Update Step, displayed in blue.
    
    \begin{figure} [H]
    \centering
    \includegraphics[scale = 0.6]{Kalman Filter Images/UKFFlowDiagram.jpg}
    \caption{A flow chart depicting one iteration of the UKF process. The sigma points are shaded in yellow, the intermediate steps in red, the Kalman gain in green, and the posterior estimates in blue.}
    \label{fig:UKF_Theory_FlowChart}
    \end{figure}
    
    \subsection{Stability and Divergence Issues}
    When utilizing the UKF, it is important to consider the issue of positive definitiness of the covariance matrix $P_k$. If this matrix becomes non-positive definite, the filter can experience severe divergence issues due to invalid covariance matrices, which can occur as a result of the update made in equation \ref{eq:27ukf}, due to the covariance to becoming non positive-definite. This then can lead the filter to diverge \cite{SimonHaykinText}. \\
    \\
    The solution to the issue lies in using the \textbf{Cholesky factorization} \cite{SimonHaykinText}.
    \\
    \\
    At every iteration of the Kalman Filter, one must perform the following step:
    \begin{equation}
    P_k = {P_k}^{1/2} {P_k}^{T/2},
    \end{equation}
    where ${P_k}^{1/2}$ is the \textbf{Cholesky factor} and is in lower-triangular form, and ${P_k}^{T/2}$ is the Cholesky factor's transpose. Here, $P_k$ is now the product of a square matrix and its transpose, which is \textbf{guaranteed to be positive definite}. In order to find the Cholesky factor, a function such as Matlab's \emph{chol} (https://www.mathworks.com/help/matlab/ref/chol.html) can be used. By using this approach, one will have a better chance of maintaining the stability of the Kalman Filter across iterations.\\
    \\
    
\begin{tcolorbox}
\doublespacing
\textbf{\underline{The Unscented Kalman Filter Algorithm With Additive Noise}} 
\begin{enumerate}
    \item Initialize $\hat{x}_0 = E[x_0]$ or with best guess.
    \item Initialize $P_0 = E[(x_0 - \hat{x}_0)(x_0 - \hat{x}_0)^T]$ or with best guess.
    \item For each incoming data point perform the following:
    \begin{enumerate}
        \item Projection Step
        \begin{enumerate}
            \item Create Sigma Points 
            \begin{itemize}
                \item  $\chi_0 = \bar{x}$
                \item $ \chi_i = \bar{x} + (\sqrt{(L + \lambda)P_x})_i \ i = 1 ... L $
                \item $ \chi_i = \bar{x} - (\sqrt{(L + \lambda)P_x})_{i - L} \ i = L + 1 \  ...  \ 2L + 1 $
            \end{itemize}
            \item Project Sigma Points forward with $\chi_{k|k-1} = F(\chi_{k-1})$
            \item Calculate prior state estimate $ \hat{x}_k^- = \sum_{i = 0}^{2L} W_i^{(m)} \chi_{i, k|k-1}$
            \item Calculate prior covariance estimate $ P_k^- = \sum_{i = 0}^{2L} W_i^{(c)} [\chi_{i, k|k-1} - \hat{x}_k^-][\chi_{i, k|k-1} - \hat{x}_k^-]^T + Q$
            \item Project observables forward with $Y_{k|k-1} = H[\chi_{k|k-1}]$
            \item Calculate prior observable estimate $\hat{y}_k^- = \sum_{i=0}^{2L} W_i^{(m)} Y_{i,k|k-1}$
        \end{enumerate}
        \item Update Step
        \begin{enumerate}
            \item Calculate innovation covariance $P_{\tilde{y}_k, \tilde{y}_k} = \sum_{i=0}^{2L} W_i^{(c)} [Y_{i,k|k-1} - \hat{y}_k^-][Y_{i,k|k-1} - \hat{y}_k^-]^T + R$
            \item Calculate state and observable covariance $ P_{{x}_k,{y}_k} = \sum_{i=0}^{2L} W_i^{(c)} [\chi_{i,k|k-1} - \hat{x}_k^-][Y_{i,k|k-1} - \hat{y}_k^-]^T$
            \item Calculate Kalman Gain $K_k = P_{x_k, y_k} P_{\tilde{y}_k, \tilde{y}_k}^{-1}$
            \item Calculate posterior state estimate $\hat{x}_k = \hat{x}_k^- + K_k(y_k - \hat{y}_k^-)$
            \item Calculate posterior state covariance $P_k = P_k^-  - K_k P_{\tilde{y}_k, \tilde{y}_k} K_k^T$
            \item Perform Choletsky Factorization $P_k = {P_k}^{1/2} {P_k}^{T/2}$
        \end{enumerate}
        \item Set $k$ to $k + 1$ and proceed to next data point
        
        \end{enumerate}
    \end{enumerate}


\end{tcolorbox}
\singlespacing    
    
\begin{table}[H]
  \begin{center}
    \label{tab:table1}
    \begin{tabular}{c|c} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
      \textbf{Parameter} & \textbf{Definition}\\
      \hline
      \textbf{$\hat{x}_0$} & Initial state guess\\
      \hline
      \textbf{$P_0$} & Initial state covariance guess\\
      \hline
      \textbf{$\chi$} & Matrix of sigma vectors\\
      \hline
      \textbf{$L$} & Number of states\\
      \hline
      \textbf{$\lambda$} & Scaling parameter\\
      \hline
      \textbf{$F$} & Transition matrix/function\\
      \hline
      \textbf{$H$} & Measurement matrix/function\\
      \hline
      \textbf{$W$} & Unscented Transformation weights\\
      \hline
      \textbf{$Q$} & Process noise covariance\\
      \hline
      \textbf{$R$} & Measurement noise covariance\\
      \hline
      \textbf{$\hat{x}_k^-$} & Prior state estimate\\
      \hline
      \textbf{$\hat{y}_k^-$} & Prior observable estimate\\
      \hline
      \textbf{$P_k^-$} & Prior state covariance estimate\\
      \hline
      \textbf{$\tilde{y}_k$} & Innovation\\
      \hline
      \textbf{$P_{\tilde{y}_k, \tilde{y}_k}$} & Innovation covariance\\
      \hline
      \textbf{$P_{x_k, y_k}$} & State and observable covariance\\
      \hline
      \textbf{$K_k$} & Kalman Gain\\
      \hline
      \textbf{$\hat{x}_k$} & Posterior state estimate\\
      \hline
      \textbf{$P_k$} & Posterior state covariance estimate
    \end{tabular}
    \caption{Full set of notation used in the Kalman Filter equations.}
    \label{table:KF_Notation}
  \end{center}
\end{table}
    
    
\section{Parameter Estimation - Dual versus Joint}
Up to this point the UKF has been described in the sense of performing state estimation, where all the parameters of the ODE model are known. However, there are often scenarios, such as the Lotka-Volterra and T1D systems, where model parameters are unknown and need to be estimated. In order to do so, we can use the Joint and Dual UKF methods \cite{VanMereChapter}.

    \subsection{Joint}
    The Joint UKF algorithm is done very similarly to a standard state-estimation UKF. However, the parameters of the model, call them $\theta$, are now added to the state vector to make one large state vector. In essence, the parameters become states to be estimated. Thus, given states $x$ and parameters $\Theta$, our latent states vector becomes a concatenation of the two. In order to implement this, we treat the transition function related to the parameters as an identity since our goal is to estimate constant parameter values.
    
    \subsection{Dual}\\
    In the Dual UKF, two UKFs are run in parallel, one for the states and one of the parameters. The estimates for the parameters at a specific time point are then fed into the state filter, and the estimates for the states are then fed into the filter for the parameters. Thus, during implementation, two UKF functions are required, one for states and another for parameters. Once again, the transition function for the parameters should be set to an identity matrix.\\

The details of these two implementations are discussed in the \emph{Kalman Filter Implementation} portion of this work.
    
    
    
    
    %\subsection{When to Use Each}
    %The major difference between the Joint and Dual UKFs is that the Joint UKF takes into account the covariances between states and parameters. Therefore, if there are strong covariances between the states and parameters, then the Joint UKF is more useful. However, if there is not, than this can cause additional noise in the Joint UKF and the Dual UKF is the better choice.


\end{document}